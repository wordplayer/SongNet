{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook implements the following workflow for both the Erdos an CitNet datasets \n",
    "\n",
    " 1. Load in data as a networkX graph\n",
    " 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-louvain\n",
    "# !pip install networkx\n",
    "# !pip install python-igraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "CPU times: user 3.22 ms, sys: 15 µs, total: 3.24 ms\n",
      "Wall time: 3.11 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pylab inline\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import scipy\n",
    "import warnings\n",
    "\n",
    "import community as community_louvain\n",
    "import igraph\n",
    "\n",
    "import operator\n",
    "\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "from scipy import misc\n",
    "import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import os, os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for Unzipping songnet compressed. Run once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gzip\n",
    "\n",
    "# #Define the file's location\n",
    "# file_path = './playlist-songnet-compressed.gz'\n",
    "\n",
    "# #Open the file and read its contents\n",
    "# with gzip.open(file_path, \"rb\") as file:\n",
    "#     file_content = file.read()\n",
    "\n",
    "\n",
    "# #Save the new txt file\n",
    "# txt_file_name = \"songnet.txt\"\n",
    "\n",
    "# with open(txt_file_name, \"w\") as file:\n",
    "#     file.write(file_content)\n",
    "\n",
    "# # import gzip\n",
    "# # with gzip.open('./playlist-songnet-compressed.gz', 'rb') as f, open('songnet.txt', 'w') as f_out:\n",
    "# #     f_out.write(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Validation spectrograms and reduced_songnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_path = '/datasets/home/21/321/ee228sp20ta1/G51/val_specs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of nodes are= 4280\n",
      "No. of edges are= 429918\n"
     ]
    }
   ],
   "source": [
    "file_name=\"./reduced_songnet.txt\"\n",
    "\n",
    "songs=nx.read_edgelist(file_name,create_using=nx.DiGraph())\n",
    "node, edge=songs.order(),songs.size()\n",
    "print(\"No. of nodes are=\",node)\n",
    "print(\"No. of edges are=\",edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to find largest connected component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_largest_component(generator):\n",
    "    sub_graphs = []\n",
    "    for item in generator:\n",
    "        sub_graphs.append(item)\n",
    "\n",
    "    list_of_all_subgraphs = [(graph, len(graph.nodes)) for graph in sub_graphs]\n",
    "\n",
    "    largest_count = 0\n",
    "    for i in range(len(list_of_all_subgraphs)):\n",
    "        count = list_of_all_subgraphs[i][1]\n",
    "        if count > largest_count:\n",
    "            largest_count = count\n",
    "            largest_component = list_of_all_subgraphs[i][0]\n",
    "    return largest_component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Largest Connected Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_ud = songs.to_undirected()\n",
    "songs_ud_components = nx.connected_component_subgraphs(songs_ud)\n",
    "songs_largest_component = find_largest_component(songs_ud_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find All Song ID's in the spectrogram validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2763/2763 [00:00<00:00, 7400.45it/s]\n"
     ]
    }
   ],
   "source": [
    "songnet_ids = []\n",
    "for image_path in tqdm(glob.glob(val_path + \"/*.png\")):\n",
    "    \n",
    "    \n",
    "    # Load and collect song ids into list of strings\n",
    "    ID = os.path.basename(image_path).partition(\".png\")[0]\n",
    "\n",
    "    if ID in list(songs_largest_component.nodes):\n",
    "        \n",
    "        songnet_ids.append(ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove nodes from the network that aren't in the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_nodes = []\n",
    "\n",
    "for node in list(songs_largest_component.nodes):\n",
    "    \n",
    "    if node not in songnet_ids:\n",
    "        remove_nodes.append(node)\n",
    "        \n",
    "songs_largest_component.remove_nodes_from(remove_nodes)\n",
    "\n",
    "ids = list(songs_largest_component.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path, dirs, files = next(os.walk(val_path))\n",
    "# file_count = len(files)\n",
    "# print(file_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Community_louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_community = community_louvain.best_partition(songs_largest_component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_sorted = sorted_x = sorted(songs_community.items(), key=operator.itemgetter(1))\n",
    "songs_coms = dict(songs_sorted)\n",
    "\n",
    "songs_ud_degrees = songs_largest_component.degree()\n",
    "songs_ud_degrees = dict(songs_ud_degrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save out largest connected component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(songs_largest_component, songs_coms, \"community\")\n",
    "nx.set_node_attributes(songs_largest_component, songs_ud_degrees, \"degrees\")\n",
    "# nx.set_node_attributes(songs_largest_component, song_partitions, \"partitions\")\n",
    "nx.write_gml(songs_largest_component, \"songnet_lc.gml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save out as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save out community detection with louvain method\n",
    "json = json.dumps(songs_coms)\n",
    "f = open(\"val_communities_1.json\",\"w\")\n",
    "f.write(json)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run igraph community Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_lc = igraph.read('songnet_lc.gml', format=\"gml\")\n",
    "song_ptns = songs_lc.community_leading_eigenvector(clusters=16)\n",
    "song_partitions = {}\n",
    "\n",
    "for cluster in range(len(song_ptns)):\n",
    "    for node in song_ptns[cluster]:\n",
    "        idx = ids[node]\n",
    "        song_partitions[idx] = cluster\n",
    "        \n",
    "#max(song_partitions.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save out igraph communities as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out community detection with igraph\n",
    "json = json.dumps(song_partitions)\n",
    "f = open(\"val_communities_10.json\",\"w\")\n",
    "f.write(json)\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Save out to format for Gephi Visualizer software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.set_node_attributes(songs_largest_component, songs_coms, \"community\")\n",
    "# nx.set_node_attributes(songs_largest_component, songs_ud_degrees, \"degrees\")\n",
    "# nx.set_node_attributes(songs_largest_component, song_partitions, \"partitions\")\n",
    "# nx.write_gml(songs_largest_component, \"songnet_lc.gml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot  Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename = \"Erdos_partitions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compared to 'community' library, 'igraph' has more flexibilty to detect communities. Igraph allows the user to partition the network into the number of communities that the user wishes. Obviously this number is bounded. Now, you will use this feature to divide the given network into '5' communities using 'igraph' and observe the results. Write a python code to implement the above task for the Citation and the Erdos networks. Remember that unlike 'community', igraph provides multiple approaches to community detection, the most obvious approach being the greedy one because it optimizes modularity. Visualize your community detection results in Gephi for both the networks. Label the nodes in the visualization properly. Use largest connected components if required. Use different colors for nodes in every community. Include the images(.jpg, .png) of your visualizations here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO? Analyze communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# track={}\n",
    "\n",
    "# for key,value in erdos_community.items():\n",
    "#     if value not in track:\n",
    "#         track[value]=0\n",
    "#     else:\n",
    "#         track[value]+=1\n",
    "        \n",
    "# import operator\n",
    "\n",
    "# sorted_track = sorted(track.items(), key=operator.itemgetter(1))\n",
    "\n",
    "# erdos_community_counts = dict(sorted_track)\n",
    "\n",
    "# top_five = []\n",
    "# for i in range(5):\n",
    "#     largest = erdos_community_counts.popitem()    \n",
    "#     top_five.append(largest[0])\n",
    "    \n",
    "\n",
    "# top_five_com = {}\n",
    "# for community in top_five:\n",
    "#     top_five_com[community] = [x for x,y in erdos_largest_component.nodes(data=True) if y['community']==community]\n",
    "    \n",
    "# from collections import defaultdict\n",
    "# top_three_nodes_per_community = {}\n",
    "# com_with_degree = defaultdict(list)\n",
    "\n",
    "# for cm, list_nodes in top_five_com.items():\n",
    "#     test = []\n",
    "#     for node in list_nodes:\n",
    "#         degree = (erdos_largest_component.nodes[node]['author'], erdos_largest_component.nodes[node]['degrees'])\n",
    "#         test.append(degree)\n",
    "#     sorted_list = dict(sorted(test, key = lambda x: x[1]))\n",
    "    \n",
    "#     top_3 = []\n",
    "#     for i in range(3):\n",
    "#         largest = sorted_list.popitem()    \n",
    "#         top_3.append(largest)\n",
    "    \n",
    "#     com_with_degree[cm] = top_3\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
